{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal basica con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://machinelearningmastery.com/tutorial-first-neural-network-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasos del Tutorial\n",
    "\n",
    "\n",
    "    - Cargar los datos.\n",
    "    - Definir el Modelo.\n",
    "    - Compilar el Modelo.\n",
    "    - Entrenar el Modelo.\n",
    "    - Evaluar el Modelo.\n",
    "    - Hacer predicciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar los datos\n",
    "\n",
    "Importamos las librerias basicas de Keras e inicializamos el seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar un dataset de diabetes (Pima Indians onset) del repositorio de UCI Machine Learning. Describe registros medicos de pacientes Pima Indians y si ellos han declarado un inicio de diabetes dentro de los 5 años.\n",
    "\n",
    "Este es un problema tipico de clasificacion binaria (inciio de diabetes con 1 o 0 si no lo ha tenido). Todas las variables asociadas a los pacientes son numericas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos directamente con la funcion loadtxt() de NumPy. Hay 8 variables de entrada y 1 de salida (la ultima columna). Una vez cargado, podemos deividir el dataset en un array de entrada (X) y un vector de salida (Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cargamos el dataset pima indians\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definir el Modelo\n",
    "\n",
    "Los Modelos en Keras se definen como una secuencia de capas (layers).\n",
    "\n",
    "Creamos un modelo Sequential y vamos agregando capas segun necesitemos.\n",
    "\n",
    "Lo primero es asegurar que la capa de entrada tiene el numero de inputs correcto. Esto se define cuando creamos la primera capa con el argumento **input_dim** estableciendolo a 8 para este caso. \n",
    "\n",
    "How do we know the number of layers and their types?\n",
    "\n",
    "En este ejemplo usaremos una estructura de neuronas fully-connected en 3 capas hidden.\n",
    "\n",
    "Las capas \"fully-connected\" se definen usando la clase **Dense**. Especificamos el numero de neuronas por capa en el 1er argumento, el metodo de inicialización es el 2do argumento (**init**) y también especificamos la función de activación con el argumento **activation**.\n",
    "\n",
    "En este caso, inicializamos los pesos con un random pequeño generado por una distribución uniforme (**‘uniform‘**) entre 0 y 0.05 - esta es la inicialización por defecto en  Keras.\n",
    "\n",
    "Usaremos una (‘**relu**‘) como funcion de activacion en las primeras 2 capas y la sigmoide en la capa de salida. \n",
    "\n",
    "Usamos la sigmoide en la ultima capa para asegurar que la salida esta entre 0 y 1, de tal forma de asignar facilmente una probabilidad para la clase 1 o ajustar una clasificación 'dura' de cualquier clase con un umbral por defecto de 0.5.\n",
    "\n",
    "Las primera capa hidden tiene 12 neuronas y espera 8 variables de entrada. La segunda capa hidden tiene 8 neuronas y la de salida solo 1 para predecir la clase (es decir si tiene inicio de diabetes o no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creamos el modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compilar el Modelo\n",
    "\n",
    "Ahora que ya tenemos el modelo definido, vamos a compilarlo.\n",
    " \n",
    "La compilación del modelo utiliza las librerias que hay en el backend de Keras (Theano o TensorFlow). El backend elige la mejor manera de representar la red teniendo en cuenta el hardware (CPU, GPU, etc.) y la plataforma usada (podria ser distribuída).\n",
    "\n",
    "Cuando compilamos debemos especificar algunos parametros adicionales requeridos para entrenar la red. Entrenar la red significa encontrar los pesos que optimicen la función coste y de esta manera hacer las predicciones del problema.\n",
    "\n",
    "Ademas, debemos especificar la función de coste (loss function) que nos permita evaluar  el conjunto de pesos respecto de la variable de salida conocida y también el 'optimizador' que nos permita ajustar poco a poco estos pesos.\n",
    "\n",
    "En este caso usaremos como función de coste una pérdida logarítmica, que para la clasificación binaria es la “**binary_crossentropy**“. \n",
    "\n",
    "Como método de optimización usaremos un derivado del descenso de gradiente estocatico: “**adam**”, que además es el método por default en Keras.\n",
    "\n",
    "Finalmente,y dado que es un problema de clasificación, utilizaremos el **accuracy** como metrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenar el Modelo\n",
    "\n",
    "Como en otros casos, para entrenar el modelo usaremos la función **fit()**.\n",
    "\n",
    "El proceso de entrenamiento se ejecutará un número fijo de veces a través de todo el dataset. Esto se especifica a través del parámetro **epochs**. \n",
    "\n",
    "También podemos indicar el número de instancias que deben evaluarse antes de actualizar los pesos de la red. Esto es lo que se denomina 'tamaño del batch', y se especifica a través del parámetro **batch_size**.\n",
    "\n",
    "Para este ejemplo, usaremos un número de 150 iteraciones (epochs) y un batch size de 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named mkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s - loss: 1.0972 - acc: 0.5872     \n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s - loss: 0.7484 - acc: 0.6159     \n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s - loss: 0.6884 - acc: 0.6107     \n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s - loss: 0.6697 - acc: 0.6406     \n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s - loss: 0.6397 - acc: 0.6328     \n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s - loss: 0.6273 - acc: 0.6615     \n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s - loss: 0.6244 - acc: 0.6667     \n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s - loss: 0.6293 - acc: 0.6549     \n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s - loss: 0.6204 - acc: 0.6667     \n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s - loss: 0.6205 - acc: 0.6641     \n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s - loss: 0.6284 - acc: 0.6719     \n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s - loss: 0.6128 - acc: 0.6797     \n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s - loss: 0.6275 - acc: 0.6719     \n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s - loss: 0.6103 - acc: 0.6706     \n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s - loss: 0.6131 - acc: 0.6771     \n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s - loss: 0.6006 - acc: 0.6862     \n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s - loss: 0.5934 - acc: 0.6966     \n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s - loss: 0.6078 - acc: 0.6771     \n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s - loss: 0.5890 - acc: 0.6940     \n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s - loss: 0.5759 - acc: 0.7044     \n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s - loss: 0.5846 - acc: 0.7005     \n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s - loss: 0.5933 - acc: 0.6654     \n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s - loss: 0.5885 - acc: 0.7005     \n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s - loss: 0.5822 - acc: 0.7005     \n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s - loss: 0.5757 - acc: 0.7174     \n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s - loss: 0.5774 - acc: 0.6992     \n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s - loss: 0.5687 - acc: 0.7122     \n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s - loss: 0.5623 - acc: 0.7396     \n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s - loss: 0.5835 - acc: 0.7057     \n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s - loss: 0.5728 - acc: 0.7148     \n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s - loss: 0.5790 - acc: 0.7005     \n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s - loss: 0.5659 - acc: 0.7148     \n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s - loss: 0.5566 - acc: 0.7292     \n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s - loss: 0.5622 - acc: 0.7292     \n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s - loss: 0.5628 - acc: 0.7135     \n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s - loss: 0.5685 - acc: 0.7201     \n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s - loss: 0.5449 - acc: 0.7331     \n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s - loss: 0.5503 - acc: 0.7240     \n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s - loss: 0.5581 - acc: 0.7318     \n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s - loss: 0.5518 - acc: 0.7331     \n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s - loss: 0.5516 - acc: 0.7305     \n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s - loss: 0.5471 - acc: 0.7174     \n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s - loss: 0.5479 - acc: 0.7422     \n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s - loss: 0.5412 - acc: 0.7383     \n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s - loss: 0.5385 - acc: 0.7448     \n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s - loss: 0.5429 - acc: 0.7422     \n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s - loss: 0.5362 - acc: 0.7331     \n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s - loss: 0.5471 - acc: 0.7370     \n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s - loss: 0.5343 - acc: 0.7409     \n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s - loss: 0.5361 - acc: 0.7487     \n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s - loss: 0.5339 - acc: 0.7448     \n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s - loss: 0.5392 - acc: 0.7422     \n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s - loss: 0.5360 - acc: 0.7500     \n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s - loss: 0.5455 - acc: 0.7279     \n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s - loss: 0.5268 - acc: 0.7513     \n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s - loss: 0.5382 - acc: 0.7435     \n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s - loss: 0.5329 - acc: 0.7383     \n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s - loss: 0.5277 - acc: 0.7396     \n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s - loss: 0.5206 - acc: 0.7461     \n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s - loss: 0.5401 - acc: 0.7474     \n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s - loss: 0.5228 - acc: 0.7279     \n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s - loss: 0.5204 - acc: 0.7474     \n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s - loss: 0.5438 - acc: 0.7370     \n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s - loss: 0.5346 - acc: 0.7318     \n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s - loss: 0.5187 - acc: 0.7461     \n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s - loss: 0.5177 - acc: 0.7487     \n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s - loss: 0.5260 - acc: 0.7409     \n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s - loss: 0.5192 - acc: 0.7357     \n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s - loss: 0.5219 - acc: 0.7331     \n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s - loss: 0.5391 - acc: 0.7201     \n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s - loss: 0.5239 - acc: 0.7487     \n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s - loss: 0.5252 - acc: 0.7617     \n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s - loss: 0.5176 - acc: 0.7500     \n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s - loss: 0.5130 - acc: 0.7643     \n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s - loss: 0.5105 - acc: 0.7565     \n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s - loss: 0.5107 - acc: 0.7487     \n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s - loss: 0.5183 - acc: 0.7578     \n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s - loss: 0.5152 - acc: 0.7435     \n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s - loss: 0.5198 - acc: 0.7552     \n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s - loss: 0.5077 - acc: 0.7578     \n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s - loss: 0.5085 - acc: 0.7591     \n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s - loss: 0.5074 - acc: 0.7474     \n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s - loss: 0.5094 - acc: 0.7500     \n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s - loss: 0.4978 - acc: 0.7617     \n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s - loss: 0.5093 - acc: 0.7578     \n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s - loss: 0.5136 - acc: 0.7526     \n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s - loss: 0.5029 - acc: 0.7591     \n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s - loss: 0.4988 - acc: 0.7630     \n",
      "Epoch 89/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s - loss: 0.5074 - acc: 0.7734     \n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s - loss: 0.5159 - acc: 0.7552     \n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s - loss: 0.4999 - acc: 0.7565     \n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s - loss: 0.5150 - acc: 0.7331     \n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s - loss: 0.4952 - acc: 0.7604     \n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s - loss: 0.4977 - acc: 0.7630     \n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s - loss: 0.5018 - acc: 0.7409     \n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s - loss: 0.4919 - acc: 0.7669     \n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s - loss: 0.4906 - acc: 0.7826     \n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s - loss: 0.4886 - acc: 0.7760     \n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s - loss: 0.4943 - acc: 0.7643     \n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s - loss: 0.4899 - acc: 0.7695     \n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s - loss: 0.4951 - acc: 0.7643     \n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s - loss: 0.5044 - acc: 0.7487     \n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s - loss: 0.4992 - acc: 0.7695     \n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s - loss: 0.5042 - acc: 0.7604     \n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s - loss: 0.5167 - acc: 0.7578     \n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s - loss: 0.4918 - acc: 0.7734     \n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s - loss: 0.4974 - acc: 0.7682     \n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s - loss: 0.5021 - acc: 0.7617     \n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s - loss: 0.4922 - acc: 0.7552     \n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s - loss: 0.4973 - acc: 0.7539     \n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s - loss: 0.4901 - acc: 0.7630     \n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s - loss: 0.4965 - acc: 0.7578     \n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s - loss: 0.5038 - acc: 0.7578     \n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s - loss: 0.5018 - acc: 0.7474     \n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s - loss: 0.4960 - acc: 0.7604     \n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s - loss: 0.5021 - acc: 0.7617     \n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s - loss: 0.4957 - acc: 0.7617     \n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s - loss: 0.4887 - acc: 0.7708     \n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s - loss: 0.4885 - acc: 0.7760     \n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s - loss: 0.4977 - acc: 0.7721     \n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s - loss: 0.4893 - acc: 0.7812     \n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s - loss: 0.4911 - acc: 0.7656     \n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s - loss: 0.4856 - acc: 0.7604     \n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s - loss: 0.4821 - acc: 0.7760     \n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s - loss: 0.4921 - acc: 0.7747     \n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s - loss: 0.4837 - acc: 0.7812     \n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s - loss: 0.4933 - acc: 0.7539     \n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s - loss: 0.4784 - acc: 0.7656     \n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s - loss: 0.4863 - acc: 0.7604     \n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s - loss: 0.4808 - acc: 0.7734     \n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s - loss: 0.4928 - acc: 0.7526     \n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s - loss: 0.4912 - acc: 0.7721     \n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s - loss: 0.4935 - acc: 0.7591     \n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s - loss: 0.4920 - acc: 0.7565     \n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s - loss: 0.4793 - acc: 0.7721     \n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s - loss: 0.4789 - acc: 0.7721     \n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s - loss: 0.4747 - acc: 0.7865     \n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s - loss: 0.4811 - acc: 0.7812     \n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s - loss: 0.4769 - acc: 0.7643     \n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s - loss: 0.4771 - acc: 0.7812     \n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s - loss: 0.4781 - acc: 0.7721     \n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s - loss: 0.4895 - acc: 0.7565     \n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s - loss: 0.4829 - acc: 0.7695     \n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s - loss: 0.4793 - acc: 0.7747     \n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s - loss: 0.5089 - acc: 0.7409     \n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s - loss: 0.4895 - acc: 0.7630     \n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s - loss: 0.4936 - acc: 0.7539     \n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s - loss: 0.4754 - acc: 0.7734     \n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s - loss: 0.4879 - acc: 0.7578     \n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s - loss: 0.4832 - acc: 0.7630     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faeb83ab110>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos el modelo...\n",
    "model.fit(X, Y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluar el Modelo\n",
    "\n",
    "Una vez entrenado el modelo, ahora podemos evaluar si los pesos se han ajustado correctamente.\n",
    "\n",
    "En este caso lo vamos a medir utilizando los datos de entrenamiento (lo cual no es correcto, pero nos dará una idea). Esto no nos dará información sobre la calidad del modelo en datos que no ha visto. Idealmente lo que hay que hacer es separar un porcentaje de los datos de entrenamiento par validación o testing. \n",
    "\n",
    "Para evaluar el modelo usaremos la función **evaluate()** y, como hemos dicho, en este caso le pasaremos los datos de entrada X y el de salida Y.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/768 [>.............................] - ETA: 0s\n",
      "acc: 78.26%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hacer predicciones\n",
    "\n",
    "    Una vez entrenado el modelo, como puedo hacer predicciones sobre nuevos datos?\n",
    "\n",
    "Para hacer predicciones podemos usar la función **model.predict()**. \n",
    "\n",
    "predict(self, x, batch_size=32, verbose=0)\n",
    "\n",
    "Siendo\n",
    "\n",
    "- x: the input data, as a Numpy array (or list of Numpy arrays if the model has multiple outputs).\n",
    "- batch_size: integer.\n",
    "- verbose: verbosity mode, 0 or 1.\n",
    "\n",
    "De tal forma que si los nuevos datos los introducimos en la matriz N, quedaria:\n",
    "\n",
    "prediccion = model.predict(N, batch_size=10,verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
