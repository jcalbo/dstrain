{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "- Procesamiento de Imagenes\n",
    "- A. Como enfocan el problema las Convolutional Neural Networks\n",
    "- B. Una interpretación de la Convolución\n",
    "- C. Conceptos de CNN\n",
    "    - C.1 El problema del tamaño de salida\n",
    "    - C.2 Padding y Stride\n",
    "    - C.3 Pooling\n",
    "    - C.4 Capas de Dropout\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de Imágenes\n",
    "\n",
    "Las Convolutional Neural Networks (CNN) son una mezcla de Redes Neuronales en el sentido clásico (utilizando los mecanismos de feedforward y backpro) y de operaciones de **convolución** en el sentido matemático. Aunque pueden utilizarse en múltiples problemas, el caso de uso mas exitoso ha sido el del Procesamiento de Imagenes, en particular la clasificación e identificación de imágenes.\n",
    "\n",
    "La *clasificación de imágenes* es la proceso de tomar una imagen de entrada y devolver una clase (un gato, perro, etc) o una probabilidad de clases que mejor se asocie a la imagen. \n",
    "\n",
    "Para los seres humanos, esta tarea de reconocimiento es una de las primeras habilidades que aprendemos. En general, somos capaces de identificar rápida y fácilmente el entorno en el que estamos, así como los objetos que nos rodean. Cuando vemos una imagen o simplemente cuando miramos el mundo que nos rodea, la mayoría de las veces somos capaces de caracterizar inmediatamente la escena y dar a cada objeto una etiqueta. Estas habilidades de clasificación no son tan evidentes ni sencillas para los computadores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/Corgi3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando una computadora **\"ve\"** una imagen, lo que ve es una matriz de píxeles (es decir, valores numéricos). Dependiendo de la resolución y el tamaño de la imagen, verá (por ej.) una matriz de 32 x 32 x 3 de números (pixels). \n",
    "\n",
    "- Los 2 primeros representan la cantidad de elementos que constituyen el ancho y alto de la imágen. En este caso es una imágen cuadrada representada por 32 pixels de ancho y 32 de alto, es decir 1024 números.\n",
    "- El tercer valor representa el color o colores que se asocian a cada uno de los 1024 pixels.\n",
    "\n",
    "Digamos que tenemos una imagen en color en formato JPG y su tamaño es de 480 x 480. La matriz representativa será de 480 x 480 x 3. Cada uno de estos números tiene un valor de 0 a 255, que describe la intensidad de píxel en ese punto. Estos números son las únicas entradas disponibles para el ordenador.\n",
    "\n",
    "La idea es que el proceso reciba esta matriz de números y devuelva valores que describen la probabilidad de que la imagen sea una clase determinada (0.80 para el gato, 0.15 para el perro, 0.05 para el pájaro, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Como enfocan el problema las Convolutional Neural Networks\n",
    "\n",
    "Ahora que el problema está descripto a partir de las entradas y las salidas esperadas, lo que queremos es que el modelo sea capaz de procesar todas las imágenes (etiquetadas) y **extraiga las características únicas** que hacen que un perro sea un perro o que hacen que un gato sea gato. \n",
    "\n",
    "Este es un proceso similar al que seguirían nuestras mentes, casi de manera inconsciente, para abstraer el concepto de 'perro'. Cuando miramos una foto de un perro, podemos clasificarla como tal si la imagen tiene características identificables como patas o su cola. \n",
    "\n",
    "De manera similar, el ordenador puede realizar la clasificación de imágenes buscando características de bajo nivel, como aristas y curvas, y luego construyendo conceptos más abstractos a través de una serie de capas convolucionales. \n",
    "\n",
    "La idea entonces es  construir una red neuronal (de varias capas) que realicen una serie de funciones de tal forma que pueda 'aprender' estas características de bajo nivel. Mirando mas en detalle, se ejecutarán convoluciones, transformaciones lineales y/o no-lineales, operaciones de pooling, y clasificaciones a través de funciones sigmoide o softmax.\n",
    "\n",
    "![](images/nn.4244-F1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Una interpretación de la convolución\n",
    "\n",
    "La primera capa de una CNN es siempre una capa convolucional. En el caso de MNIST, la entrada es una matriz de 32 x 32 x 3 (alto, ancho, profundidad) de valores de píxeles. Ahora, para interpretar la convolución podemos imaginar una linterna que está apuntando la parte superior izquierda de la imagen cubriendo un área de 5 x 5. Y ahora, imaginemos que esta linterna se desliza poco a poco por todas las áreas de la imagen de entrada. \n",
    "\n",
    "En términos de *machine learning*, esta linterna se llama **filtro** o **kernel** y la región iluminada se llama el **campo receptivo**. Este filtro es también una matriz de números (en términos de las NN son los pesos). \n",
    "\n",
    "Una nota muy importante es que la profundidad de este filtro tiene que ser la misma que la profundidad de la entrada (en este caso sería 3), por lo que las dimensiones de este filtro es de 5 x 5 x 3. \n",
    "\n",
    "Ahora, considerando la primera posición en la que se encuentra el filtro, por ejemplo. \n",
    "\n",
    "![](images/ActivationMap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sería la esquina superior izquierda. A medida que el filtro se desliza, la **convolución** no es mas que la multiplicación elemento a elemento de los valores 'iluminados' por el filtro (*element wise multiplications*).\n",
    "\n",
    "Estas multiplicaciones (5x5x3=75 en total) se suman todas y obtenemos un único número que representa a la convolución del filtro en la posición (1,1), parte superior izquierda de la imagen. \n",
    "\n",
    "Ahora, repetimos este proceso para cada ubicación de la imagen de entrada. El próximo paso sería mover el filtro a la derecha por 1 unidad (1,2) y repetir la misma operación hasta recorrer las 32x32 posiciones. \n",
    "\n",
    "Es importante tener claro que cada pixel de entrada genera una salida en la convolución con el filtro.\n",
    "\n",
    "Otro ejemplo de convolución discreta es el siguiente:\n",
    "\n",
    "![kernel](images/theano_kernel.png)\n",
    "![](images/theano_numerical_convolution.gif)\n",
    "\n",
    "Aqui se puede ver la salida (en verde) de aplicar el kernel indicado a la imagen de entrada en azul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Conceptos de CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1 El problema del tamaño de salida\n",
    "\n",
    "Después de deslizar el filtro sobre todas las ubicaciones, descubriremos que lo que le queda es una matriz de 28 x 28 x 1 de números.\n",
    "\n",
    "La razón por la que se obtiene una matriz de 28 x 28 es que hay 784 ubicaciones diferentes que un filtro de 5 x 5 puede caber en una imagen de 32 x 32 de entrada. Estos números 784 se asignan a una matriz 28 x 28.\n",
    "\n",
    "Por ejemplo en la imagen siguiente:\n",
    "\n",
    "![](images/convolucion.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos una imagen de entrada de 7x7 y un kernel de 3x3. Cuál sería el tamaño de la salida de la convolución?\n",
    "\n",
    "En este caso, si colocamos el filtro de tal forma que la posición (1,1) del filtro coincida con la posición (1,1) de la imagen, podemos desplazar el filtro a la derecha 4 veces sin salirnos de la imagen. \n",
    "\n",
    "Al ser una imagen cuadrada, lo mismo nos sucederá al desplazarnos verticalmente, por lo que obtendríamos una salida para cada una de las 4x4 posiciones posibles del filtro (sin salirnos de la imágen). \n",
    "\n",
    "Por lo tanto, para una convolución de una imagen de 7x7 con un kernel de 3x3 es de 4x4.Para el caso anterior, una convolucion de una imagen de 32x32 con un kernel de 5x5 nos da una salida de 28x28.\n",
    "\n",
    "La formula que se aplica para determinar el tamaño de la salida para el caso de las dimensiones ancho y alto de una imágen sería:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "dimension = \n",
    "\\frac{I - K }{stride} + 1\n",
    "\\end{equation*}\n",
    "\n",
    "- I representa el tamaño de la entrada en esa dimensión\n",
    "- K representa el tamaño del kernel en esa dimensión\n",
    "- stride - lo definiremos mas adelante, pero reresenta el salto que hace el filtro (de momento es 1)\n",
    "\n",
    "Entonces si tenemos un I=32, un K=5 y un stride=1, entonces la salida para esa dimensión será:\n",
    "\n",
    "\\begin{equation*}\n",
    "dimension = \n",
    "\\frac{32 - 5 }{1} + 1 = 28\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.2 Padding y Stride\n",
    "\n",
    "Como hemos visto antes, cuando aplicamos el kernel alineado con la posición (1,1) de la imagen de entrada, y **no** nos salimos de esta imagen, el resultado de la convolución genera una salida más pequeña que la imagen original. Si aplicamos varias convoluciones en cadena obtendremos una reducción de tamaño importante.\n",
    "\n",
    "Podriamos por tanto aplicar el kernel de manera que nos 'salgamos' de la imágen, por ejemplo si el kernel es de 3x3 podriamos alinear la posición (2,2) del kernel en la posición (1,1) de la imagen. Esto supone que habría varios componentes del filtro fuera de la imagen y por tanto habría que definir esta operación. \n",
    "\n",
    "Lo más sencillo sería aplicar 'ceros' alrededor de la matriz imágen, de tal forma de que el filtro pueda aplicar la multiplicación sobre estos pixels sintéticos e inexistentes. \n",
    "\n",
    "![](images/arbitrary_padding_no_strides.gif)\n",
    "\n",
    "El **zero-padding** podriamos definirlo como un relleno de ceros en el espacio perimetral de la imagen de entrada de tal forma de poder aplicar el filtro en los bordes de la imagen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digamos que queremos aplicar la misma capa convolucional al ejemplo inicial de 32 x 32 x 3, pero queremos que la salida permanezca del mismo tamaño. Para hacer esto, podemos aplicar un **zero-padding** de tamaño 2 a esa capa. \n",
    "\n",
    "Este padding de 2 rellenará la imagen de entrada con ceros alrededor del borde dejándonos una imagen de entrada de 36 x 36 x 3.\n",
    "\n",
    "![](images/ZeroPad_2b.png)\n",
    "\n",
    "Entonces si tenemos ahora una I=36, un K=5 y un stride=1, entonces la salida para esa dimensión será:\n",
    "\n",
    "\\begin{equation*}\n",
    "dimension = \n",
    "\\frac{I - K }{stride} + 1\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "dimension = \n",
    "\\frac{36 - 5 }{1} + 1 = 32\n",
    "\\end{equation*}\n",
    "\n",
    "Lo que nos dejaría unas dimensiones de salida de 32 x 32 x 3.\n",
    "\n",
    "\n",
    "Incluyendo la variable de *zero-padding*, la formula para el cálculo de la dimensión de salida sería:\n",
    "\n",
    "\\begin{equation*}\n",
    "dimension = \n",
    "\\frac{I - K + 2P}{stride} + 1\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "dimension = \n",
    "\\frac{32 - 5 + 2x2}{1} + 1 = 32\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Por qué usar padding? \n",
    "\n",
    "Además de los beneficios respecto de las dimensiones espaciales de la imagen despues de la convolución, el padding mejora el performance de los resultados. Si las capas de convolución no realizan el zero-padding sobre las entradas, entonces el tamaño de las matrices de salida se reducirían despues de cada convolución, y la información en los bordes se perdería rapidamente.\n",
    "\n",
    "#### Por qué usar stride=1 en las convoluciones? \n",
    "\n",
    "Strides mas pequeños funcionan mejor en la practica. Adicionalmente, **stride=1** permite que dejemos el *down-sampling espacial* a las capas de POOLING, mientras que las capas de *convolución* solo harían la transformación de la entrada en la dimensión de *'profundidad'*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.3 Pooling\n",
    "\n",
    "Despues de aplicar convoluciones y alguna transformación lineal (ReLu), pueden aplicarse **capas de pooling** para hacer una *'reducción de dimensiones' o downsampling* de la imagen.\n",
    "\n",
    "En este tipo de transformación, suele haber varias técnicas disponibles, aunque la más habitual es la de **maxpooling**, que elige el mayor valor que ha resultado de aplicar el filtro de 2x2 y con un stride=2.\n",
    "\n",
    "\n",
    "![maxpooling](images/MaxPool.png)\n",
    "\n",
    "\n",
    "El resultado será una matriz de dimensiones mas reducidas pero que conserva parcialmente la información de los puntos de una región de 2x2.\n",
    "\n",
    "Otras opciones son el **average pooling** y el **pooling de norma L2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por qué aplicamos pooling?\n",
    "\n",
    "El razonamiento intuitivo de las **capas de pooling** es que, una vez que sabemos que una característica específica está en la imagen de entrada (y por tanto habrá un alto valor de activación), su ubicación exacta no es tan importante como su ubicación relativa a las otras características. Como se puede ver, esta capa reduce drásticamente la dimensión espacial de la matriz de entrada(en altura y en ancho, pero no la profundidad).\n",
    "\n",
    "Esto sirve para 2 propósitos: \n",
    "\n",
    "- El primero es que el número de parámetros o pesos se reduce en un 75%, y por lo tanto el esfuerzo computacional también se reduce.\n",
    "\n",
    "- El segundo es que también controlamos el overfitting. Esto supone un mecanismo de regularización que permite generalizar el modelo. Un síntoma típico de overfitting es obtener valores muy altos de accuracy en training y muy bajos en test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.4 Capas de Dropout\n",
    "\n",
    "El **dropout** es una técnca por la cual ciertas neuronas aleatoriamente son 'olvidadas' (ignoradas)  durante el entrenamiento. Esto significa que su contribución a la activación de las neuronas en las capas siguientes es nula y por lo tanto no se produce una actualización de los pesos asociados durante la fase de \"backpropagation\".\n",
    "\n",
    "A medida que una red neuronal 'aprende', los pesos asociados a las neuronas se van acomodando en su contexto dentro de la red. Los pesos pueden interpretarse como ajustes que resaltan características concretas y que proporcionan cierta especialización. Las neuronas vecinas se ven influenciadas por esta especialización, pero si esta influencia se lleva demasiado lejos puede generar un modelo frágil y demasiado especializado para los datos de entrenamiento (overfitting). Esta dependencia que tienen las neuronas del contexto durante el entrenamiento se denomina **complex co-adaptations**.\n",
    "\n",
    "Para combatir este problema de overfitting, las capas de **dropout** seleccionan aleatoriamente un conjunto de neuronas y simplemente las pone a 0.\n",
    "\n",
    "Segun avanzan las **epochs** del training, si algunas neuronas son 'apagadas' aleatoriamente, el resto de neuronas tendrán que alcanzar la representación necesaria para hacer predicciones correctas, aún cuando han desaparecido algunas. Se cree que esta adaptación resulta en múltiples representaciones internas independientes que la red ha aprendido.\n",
    "\n",
    "El efecto entonces es que la NN se vuelve menos sensible a los pesos específicos de las neuronas. Por lo tanto provoca que la red generalice mejor y es menos propensa al overfitting.\n",
    "\n",
    "Es importante tener en cuenta que esta capa sólo se utiliza durante el **entrenamiento**, y no durante la etapa de **test**.\n",
    "\n",
    "Ver el paper de G.Hinton y otros:\n",
    "https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
