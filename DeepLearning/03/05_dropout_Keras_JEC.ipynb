{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Regularization in Deep Learning Models With Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptado de https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "\n",
    "El Dropout es una técnica de regularización simple y poderosa que se aplica en Redes Neuronales y Deep Learning.\n",
    "\n",
    "Los objetivos de este ejercicio son:\n",
    "\n",
    "- Ver como funciona la técnica de dropout en Keras\n",
    "- Descripción del modelo a usar\n",
    "- Como usar dropout en la capa de entrada\n",
    "- Como usar dropout en las capas hidden\n",
    "- Notas para el uso de dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como utilizar Dropout en Keras\n",
    "\n",
    "Ver el paper original de Srivastava y otros (2014): [A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/v15/srivastava14a.html)\n",
    "\n",
    "En Keras el dropout se implementa facilmente seleccionando los nodos que serán ignorados (con una probabilidad dada) en el ciclo de actualización de los pesos.\n",
    "\n",
    "El Dropout se utiliza solamente en la fase de entrenamiento del modelo. Por lo tanto no se aplica cuando se evalúa la capacidad del modelo.\n",
    "\n",
    "\n",
    "### Descripción del modelo a usar\n",
    "\n",
    "Para el ejemplo usaremos el [dataset de Sonar](http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/), que representa un problema de **clasificación binaria** donde hay que clasificar rocas e imitaciones de rocas según la señal devuelta por un sonar. Este es un buen dataset para NN ya que la mayoría de sus datos son numéricos y están a escala.\n",
    "\n",
    "Se implementará un modelo basado en scikit-learn con una cross validation 10-fold, y se evaluaran los resultados de la red.\n",
    "\n",
    "Por cada muestra de datos hay 60 features de entrada y 1 único valor de salida, y todos están normalizados antes de ser usados por la NN. Las funciones de activación en las capas hidden serán **relu** y la de salida será (por supuesto) una **sigmoide**.\n",
    "\n",
    "El modelo de NN de referencia tiene dos capas hidden, la primera con 60 unidades y la segunda con 30. Se utilizará SGD para entrenar el modelo con un **learning-rate** y **momentum** relativamente bajos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las librerias relevantes\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(7)\n",
    "\n",
    "# Cargamos el dataset\n",
    "dataframe = read_csv(\"data/sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# separamos los predictores (X) de los targets (Y)\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "\n",
    "# codificamos los valores categoricos como enteros\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 60)\n",
      "(208,)\n"
     ]
    }
   ],
   "source": [
    "# Verificamos las dimensiones de los datos\n",
    "print X.shape\n",
    "print Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0453,  0.0523,  0.0843,  0.0689,  0.1183,  0.2583,  0.2156,\n",
       "        0.3481,  0.3337,  0.2872,  0.4918,  0.6552,  0.6919,  0.7797,\n",
       "        0.7464,  0.9444,  1.    ,  0.8874,  0.8024,  0.7818,  0.5212,\n",
       "        0.4052,  0.3957,  0.3914,  0.325 ,  0.32  ,  0.3271,  0.2767,\n",
       "        0.4423,  0.2028,  0.3788,  0.2947,  0.1984,  0.2341,  0.1306,\n",
       "        0.4182,  0.3835,  0.1057,  0.184 ,  0.197 ,  0.1674,  0.0583,\n",
       "        0.1401,  0.1628,  0.0621,  0.0203,  0.053 ,  0.0742,  0.0409,\n",
       "        0.0061,  0.0125,  0.0084,  0.0089,  0.0048,  0.0094,  0.0191,\n",
       "        0.014 ,  0.0049,  0.0052,  0.0044])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Como se ven los datos de una muestra\n",
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creacion de la red neuronal\n",
    "def crea_NN():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # Compila el modelo\n",
    "    sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del Modelo básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named mkl\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(7)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=crea_NN, \n",
    "                                          epochs=300, \n",
    "                                          batch_size=16, \n",
    "                                          verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy de la clasificación (sin dropout): 85.02% (6.04%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy de la clasificación (sin dropout): %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Como usar dropout en la capa de entrada\n",
    "\n",
    "El Dropout puede aplicarse a las neuronas de la capa de entrada o capa visible.\n",
    "\n",
    "En el ejemplo de abajo, agregamos una nueva capa de dropout entre la capa de entrada y la primera capa hidden.  \n",
    "\n",
    "Se asigna un valor de 0.2 para el dropout, lo que significa que el 20% de los valores de entrada serán aleatoriamente excluídas en cada ciclo de actualización.\n",
    "\n",
    "Adicionalmente, como se recomienda en al paper original, se impone una restricción sobre los pesos de cada capa oculta, asegurando que la norma máxima de los pesos no exceda un valor de 3. Esto se hace estableciendo el argumento *kernel_constraint* en la clase Dense al construir las capas.\n",
    "\n",
    "El \"learning-rate\" ha sido aumentado en un orden de magnitud y el valor del \"momentum\" ha sido incrementado a 0.9. Estos cambios fueron recomendados en el paper original de dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout en la capa de entrada with weight constraint\n",
    "def crea_NN_dropout_1():\n",
    "    # Creamos el modelo\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(60,)))\n",
    "    model.add(Dense(60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # Compilamos el modelo\n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=7\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=crea_NN_dropout_1,\n",
    "                                          epochs=300, \n",
    "                                          batch_size=16,\n",
    "                                          verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy de la clasificación (con dropout capa de entrada): 86.06% (4.52%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy de la clasificación (con dropout capa de entrada): %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Como usar dropout en las capas hidden\n",
    "\n",
    "El Dropout puede ser aplicado a las neuronas de las capas hidden dentro de la red neuronal.\n",
    "\n",
    "En el ejemplo de abajo, el dropout se aplica entre las 2 capas hidden y entre la última capa hidden y la capa de salida. De nuevo, se aplica un valor de dropout del 20%, así como una restricción de peso sobre dichas capas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropout en capas hidden con constricción de pesos\n",
    "def crea_NN_dropout_hidden():\n",
    "    # Creamos el modelo\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # Compilar modelo\n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=7\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "estimators = []\n",
    "\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=crea_NN_dropout_hidden, \n",
    "                                          epochs=300, \n",
    "                                          batch_size=16, \n",
    "                                          verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy de la clasificación (con dropout en capas hidden): 83.09% (5.96%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy de la clasificación (con dropout en capas hidden): %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que, para este problema y para esta configuración de NN, usando dropout en las capas hidden no mejora la performance. De hecho ha sido peor que en el caso sin dropout.\n",
    "\n",
    "Es posible que el training requiera más epochs o que se necesite otro tuning en \"learning rate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notas para el uso de Dropout\n",
    "\n",
    "El paper original de Dropout proporciona resultados experimentales en un conjunto de problemas típicos de machine learning. El paper proporciona un numero de heuristicas útiles a tener en cuenta cuando usamos dropout en la práctica.\n",
    "\n",
    "- **Usar valores de dropout de 20%-50%** (20% es un buen comienzo). Una probabilidad demasiada baja tiene un efecto mínimo y un valor demasiado alto impedirá que la red aprenda adecuadamente.\n",
    "\n",
    "- **Usar una red grande.** Se obtendrán mejores resultados con una red 'grande', dandole al modelo más opciones de aprender representaciones independientes.\n",
    "\n",
    "- **Usar dropout en las capas de entrada y en las hidden.** Aplicar el dropout en todas las capas de la NN ha mostrado buenos resultados.\n",
    "\n",
    "- **Usar valores grandes de \"learning rate\" con \"decay\" y también un valor alto de \"momentum\".** Incrementar el  learning rate por un a factor de 10 a 100 y usar valores de momentum de 0.9 o 0.99.\n",
    "\n",
    "- **Limitar los pesos de la NN**. Un  learning rate alto puede generar pesos muy grandes en la red. Imponer un límite en los pesos (tal como la regularización max-norm con un tamaño de 4 o 5) ha mostrado una mejora en los resultados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
