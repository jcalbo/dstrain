{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analisis de Sentimiento (simple)\n",
    "\n",
    "El objetivo de este ejemplo es hacer el Analisis de Sentimeinto de reseñas de películas, viendo si es positivo o negativo.\n",
    "\n",
    "Vamos a utilizar la libreria NLTK, asi como otras herramientas.\n",
    "\n",
    "- De **NLTK** usaremos el *Lemmatizer* que aplica lematización a las palabras. Simplificando mucho, un lematizador convierte las palabras a su forma básica, por ejemplo las palabras \"perro\", \"perros\" denotan lo mismo, asi que solo deja la raiz de la palabra. Esto tiene el efecto de reducir el tamaño del diccionario.\n",
    "\n",
    "- Tambien usaremos **BeautifulSoup** porque los datos vienen en XML y necesitamos extraerlos.\n",
    "\n",
    "- Por ultimo, para la clasificación usaremos el típico clasificador de **Regresión Logistica** de sklearn. La clasifiación de sentimientos es binaria, positivo o negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instanciamos un objeto de la claseWordNetLemmatizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cargamos las 'stopwords' del idioma Ingles de algun sitio de internet\n",
    "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tenemos 3 archivos con las reseñas o criticas. 2 de ellos están etiquetados como POSITIVAS o NEGATIVAS. \n",
    "\n",
    "### El 3ro está sin etiquetar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cargamos las criticas POSITIVAS\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('/home/jorge/data/sorted_data_acl/electronics/positive.review').read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dejamos solo las del campo 'review_text'\n",
    "positive_reviews = positive_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<review_text>\\nI am very happy with this product. It folds super slim, so traveling with it is a breeze! Pretty good sound - not Bose quality, but for the price, very respectable! I've had it almost a year, and it has been along on many weekend get-aways, and works great. I use it alot, so it was a good purchase for me\\n</review_text>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# por ejemplo la 10 reseña positiva sería:\n",
    "positive_reviews[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hacemos lo mismo con las Negativas\n",
    "negative_reviews = BeautifulSoup(open('/home/jorge/data/sorted_data_acl/electronics/negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<review_text>\\nI knew these were inexpensive CD cases, but I can't even open one without it breaking into two pieces..\\n</review_text>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_reviews[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tenemos muchas más revisiones POSITIVAS que NEGATIVAS, asi que tenemos un problema de balance\n",
    "\n",
    "# Tomaremos una muestra de las positivas del mismo tamaño que las negativas, \n",
    "# asi tenemos las clases balanceadas\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del Diccionario\n",
    "\n",
    "El diccionario debe incluir todas las palabras de nuestro vocabulario. \n",
    "Para ello tenemos que centrarnos en las 'palabras' que son representativas y utiles. Por lo tanto necesitamos tokenizar (eliminando distinciones entre mayusculas y minusculas) y luego lemmatizar para dejar las palabras en su forma basica.\n",
    "\n",
    "\n",
    "\n",
    "### Primero vamos a tratar de tokenizar los textos usando el tokenizador de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wish',\n",
       " u'the',\n",
       " u'unit',\n",
       " u'had',\n",
       " u'a',\n",
       " u'separate',\n",
       " u'online/offline',\n",
       " u'light',\n",
       " u'.',\n",
       " u'When',\n",
       " u'power',\n",
       " u'to',\n",
       " u'the',\n",
       " u'unit',\n",
       " u'is',\n",
       " u'missing',\n",
       " u',',\n",
       " u'the',\n",
       " u'single',\n",
       " u'red',\n",
       " u'light',\n",
       " u'turns',\n",
       " u'off',\n",
       " u'only',\n",
       " u'when',\n",
       " u'the',\n",
       " u'warning',\n",
       " u'sounds',\n",
       " u'.',\n",
       " u'The',\n",
       " u'warning',\n",
       " u'sound',\n",
       " u'is',\n",
       " u'like',\n",
       " u'a',\n",
       " u'lot',\n",
       " u'of',\n",
       " u'sounds',\n",
       " u'you',\n",
       " u'hear',\n",
       " u'in',\n",
       " u'the',\n",
       " u'house',\n",
       " u'so',\n",
       " u'it',\n",
       " u'is',\n",
       " u\"n't\",\n",
       " u'always',\n",
       " u'easy',\n",
       " u'to',\n",
       " u'tell',\n",
       " u'what',\n",
       " u'is',\n",
       " u'happening']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A ver como tokeniza la 2da critica\n",
    "t = positive_reviews[2]\n",
    "nltk.tokenize.word_tokenize(t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la tokenización está considerando distintas las palabras minusculas y mayusculas.\n",
    "\n",
    "Vamos a crear una función para tokenizar mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # separamos el string en \"palabras\" (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # eliminamos las palabras cortas, probablemente no son utiles\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # Lematizamos --> palabras en forma base\n",
    "    tokens = [t for t in tokens if t not in stopwords] # quitamos stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora si podemos crear el diccionario..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creamos un mapa word-to-index de tal manera que podamos crear nuestros vectores de frecuencias mas tarde\n",
    "# El diccionario es un conjuto de pares K:V donde K es la palabra y V el indice\n",
    "\n",
    "# Tambien guardamos la tokenizacion en un par de listas para no tenerla que hacer mas adelante.\n",
    "\n",
    "# Reseñas POSITIVAS\n",
    "word_index_map = {}  # Creamos un diccionario vacio\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = [] # Creamos un par de listas vacias\n",
    "\n",
    "for review in positive_reviews:         # recorremos las reseñas POSITIVAS\n",
    "    tokens = my_tokenizer(review.text)       # tokenizamos la reseña\n",
    "    positive_tokenized.append(tokens)        # metemos la reseña tokenizada a la lista correspondiente\n",
    "    \n",
    "    for token in tokens:                     # por cada token de la reseña\n",
    "        if token not in word_index_map:            # verifico si NO esta en el diccionario\n",
    "            word_index_map[token] = current_index       # entonces lo incluyo la palabra en el dicc y su indice\n",
    "            current_index += 1                          # e incremento el contador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hago lo mismo pero para las NEGATIVAS, pero en el mismo diccionario\n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuestro diccionario tiene: 11088 palabras.\n"
     ]
    }
   ],
   "source": [
    "print \"Nuestro diccionario tiene:\", len(word_index_map), \"palabras.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de las matrices de Entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creamos una función que transforma tokens en vectores\n",
    "\n",
    "def tokens_to_vector(tokens, label):\n",
    "\n",
    "    x = np.zeros(len(word_index_map) + 1) # creamos un vector de nulos del tamaño del diccionario\n",
    "                                          # y agregamos 1 para la etiqueta\n",
    "    \n",
    "    for t in tokens:                      # recorremos los tokens de entrada\n",
    "        i = word_index_map[t]                  # guardo el indice de la palabra actual en i\n",
    "        x[i] += 1                              # incremento el contador del vector x[] en 1\n",
    "\n",
    "    # Al final del proceso, me queda un vector con el numero de repeticiones de cada palabra en la posición que \n",
    "    # tiene en el diccionario.\n",
    "\n",
    "    # Ahora normalizamos de tal manera que todo sume 1 \n",
    "    x = x / x.sum()                       # normalizamos antes de asignar la etiqueta\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cual es el tamaño de nuestros datos de entrada??\n",
    "\n",
    "# es el numero de palabras tokenizadas de las reseñas positivas y negativas, entonces:\n",
    "\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# (N x D+1 matrix - keeping them together for now so we can shuffle more easily later\n",
    "data = np.zeros((N, len(word_index_map) + 1))  ## N x D+1 (mio)\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02272727,  0.06818182,  0.02272727, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.        ,  0.        ,  0.08333333, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esta bien que de 2 porque suma 1 los datos normalizados + 1 de la etiqueta\n",
    "data[3,:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamos con los datos que hemos preparado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hacemos shuffle de los datos y creamos datos de train/test \n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Las ultimas 100 seran de test\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring de clasificacion: 0.79\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print \"Scoring de clasificacion:\", model.score(Xtest, Ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy 1.7934926583\n",
      "time -0.807642920518\n",
      "love 1.17577946379\n",
      "returned -0.72663482898\n",
      "perfect 0.951307505762\n",
      "waste -0.927823778335\n",
      "highly 0.994149449428\n",
      "wa -1.61891731543\n",
      "support -0.848011306122\n",
      "price 2.60785214634\n",
      "lot 0.71252030231\n",
      "you 1.09301794223\n",
      "poor -0.781009226709\n",
      "month -0.874442029631\n",
      "tried -0.805179215599\n",
      "pretty 0.707550889108\n",
      "quality 1.34952535326\n",
      "speaker 0.820884704347\n",
      "ha 0.80513032191\n",
      "recommend 0.729065468829\n",
      "doe -1.16068368505\n",
      "bad -0.815918568832\n",
      "item -0.983779411579\n",
      "little 0.946090010437\n",
      "sound 0.985615740155\n",
      "n't -2.0375472747\n",
      "then -1.18894130318\n",
      "money -1.09925261759\n",
      "'ve 0.858329963811\n",
      "buy -0.784170142213\n",
      "excellent 1.38226410504\n",
      "memory 0.964715412386\n",
      "week -0.752582434415\n",
      "return -1.13320762448\n",
      "fast 0.817628195731\n"
     ]
    }
   ],
   "source": [
    "#Veamos los pesos de cada palabra\n",
    "\n",
    "threshold = 0.7\n",
    "for word, index in word_index_map.iteritems():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print word, weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
