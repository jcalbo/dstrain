{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analisis de Sentimiento (simple)\n",
    "\n",
    "El objetivo de este ejemplo es hacer un *Analisis de Sentimiento* de reseñas de películas, viendo si es positivo o negativo.\n",
    "\n",
    "Vamos a utilizar la libreria NLTK, asi como otras herramientas.\n",
    "\n",
    "- De **NLTK** usaremos el *Lemmatizer* que aplica lematización a las palabras. Simplificando mucho, un lematizador convierte las palabras a su forma básica, por ejemplo las palabras \"perro\", \"perros\" denotan lo mismo, asi que solo deja la raiz de la palabra. Esto tiene el efecto de reducir el tamaño del diccionario.\n",
    "\n",
    "- Tambien usaremos **BeautifulSoup** porque los datos vienen en XML y necesitamos extraerlos.\n",
    "\n",
    "- Por ultimo, para la clasificación usaremos el típico clasificador de **Regresión Logistica** de sklearn. La clasifiación de sentimientos es binaria, positivo o negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instanciamos un objeto de la claseWordNetLemmatizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cargamos las 'stopwords' del idioma Ingles de algun sitio de internet\n",
    "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tenemos 3 archivos con las reseñas o criticas. 2 de ellos están etiquetados como POSITIVAS o NEGATIVAS. \n",
    "\n",
    "### El 3ro está sin etiquetar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos las criticas POSITIVAS\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('/home/jorge/data/sorted_data_acl/electronics/positive.review').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<review_text>\\nI purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power.\\n\\nI feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply.\\n\\nAs always, Amazon had it to me in &lt;2 business days\\n</review_text>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_reviews.review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dejamos solo las del campo 'review_text'\n",
    "positive_reviews = positive_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<review_text>\\nI am very happy with this product. It folds super slim, so traveling with it is a breeze! Pretty good sound - not Bose quality, but for the price, very respectable! I've had it almost a year, and it has been along on many weekend get-aways, and works great. I use it alot, so it was a good purchase for me\\n</review_text>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# por ejemplo la 10ma reseña positiva sería:\n",
    "positive_reviews[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hacemos lo mismo con las Negativas\n",
    "negative_reviews = BeautifulSoup(open('/home/jorge/data/sorted_data_acl/electronics/negative.review').read())\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<review_text>\\nI knew these were inexpensive CD cases, but I can't even open one without it breaking into two pieces..\\n</review_text>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_reviews[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tenemos muchas más revisiones POSITIVAS que NEGATIVAS, asi que tenemos un problema de balance\n",
    "\n",
    "# Tomaremos una muestra de las positivas del mismo tamaño que las negativas, \n",
    "# asi tenemos las clases balanceadas\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del Diccionario\n",
    "\n",
    "El diccionario debe incluir todas las palabras de nuestro vocabulario. \n",
    "Para ello tenemos que centrarnos en las 'palabras' que son representativas y utiles. \n",
    "\n",
    "Por lo tanto necesitamos:\n",
    "\n",
    "- tokenizar (eliminando distinciones entre mayusculas y minusculas) y luego \n",
    "- lemmatizar para dejar las palabras en su forma basica.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primero vamos a tokenizar los textos usando el tokenizador de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wish',\n",
       " u'the',\n",
       " u'unit',\n",
       " u'had',\n",
       " u'a',\n",
       " u'separate',\n",
       " u'online/offline',\n",
       " u'light',\n",
       " u'.',\n",
       " u'When',\n",
       " u'power',\n",
       " u'to',\n",
       " u'the',\n",
       " u'unit',\n",
       " u'is',\n",
       " u'missing',\n",
       " u',',\n",
       " u'the',\n",
       " u'single',\n",
       " u'red',\n",
       " u'light',\n",
       " u'turns',\n",
       " u'off',\n",
       " u'only',\n",
       " u'when',\n",
       " u'the',\n",
       " u'warning',\n",
       " u'sounds',\n",
       " u'.',\n",
       " u'The',\n",
       " u'warning',\n",
       " u'sound',\n",
       " u'is',\n",
       " u'like',\n",
       " u'a',\n",
       " u'lot',\n",
       " u'of',\n",
       " u'sounds',\n",
       " u'you',\n",
       " u'hear',\n",
       " u'in',\n",
       " u'the',\n",
       " u'house',\n",
       " u'so',\n",
       " u'it',\n",
       " u'is',\n",
       " u\"n't\",\n",
       " u'always',\n",
       " u'easy',\n",
       " u'to',\n",
       " u'tell',\n",
       " u'what',\n",
       " u'is',\n",
       " u'happening']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A ver como tokeniza la 2da critica\n",
    "t = positive_reviews[2]\n",
    "nltk.tokenize.word_tokenize(t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la tokenización está considerando como distintas las palabras minusculas y mayusculas. Entonces debemos dejar todo en minusculas para evitar que el diccionario crezca mucho.\n",
    "\n",
    "Vamos a crear una función para tokenizar mejor.\n",
    "\n",
    "La funcion my_tokenizer( ) hace:\n",
    "- Pasa la reseña a minusculas\n",
    "- Tokeniza la reseña (es decir separa el parrafo en palabras)\n",
    "- Elimina las palabras de menos de 3 letras (in, a, by, ...)\n",
    "- Elimina las pabras que están en la lista de 'stopwords'\n",
    "- Devuelve una lista filtrada de palabras de la reseña"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    s = s.lower() # todo en minusculas\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # separamos el string en \"palabras\" (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # eliminamos las palabras cortas, probablemente no son utiles\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # Lematizamos --> palabras en forma base\n",
    "    tokens = [t for t in tokens if t not in stopwords] # quitamos stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora si podemos crear el diccionario..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dos': 2, 'tres': 3, 'uno': 1}\n"
     ]
    }
   ],
   "source": [
    "# Recordamos como funciona un diccionario en Python\n",
    "# El diccionario es un conjuto de pares K:V donde K es la palabra y V el indice\n",
    "diccionario ={}\n",
    "a='uno'\n",
    "b='dos'\n",
    "c='tres'\n",
    "\n",
    "# por ejemplo puedo hacer\n",
    "diccionario[a]=1\n",
    "diccionario[b]=2\n",
    "diccionario[c]=3\n",
    "\n",
    "print diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![diccionario2](diccionario2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un mapa word-to-index de tal manera que podamos crear nuestros vectores de frecuencias mas tarde.\n",
    "Tambien guardamos la tokenizacion en un par de listas para no tenerla que hacer mas adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numero de reseñas positivas procesadas=  1000\n",
      "Se han cargado hasta ahora  7566  palabras/tokens en el diccionario.\n"
     ]
    }
   ],
   "source": [
    "# Reseñas POSITIVAS\n",
    "word_index_map = {}  # Creamos un diccionario vacio\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = [] # Creamos un par de listas vacias\n",
    "\n",
    "i=0 # cuento tokens\n",
    "\n",
    "\n",
    "\n",
    "### OJO: positive_tokenized es una lista de listas. Es decir cada reseña tokenizada\n",
    "###     crea una sublista dentro de la lista de reseñas positivas. Lo mismo luego con las negativas.\n",
    "  \n",
    "\n",
    "for review in positive_reviews:         # recorremos la lista de las reseñas POSITIVAS\n",
    "    tokens = my_tokenizer(review.text)       # tokenizamos el texto de la reseña (devuelve lista)\n",
    "    positive_tokenized.append(tokens)        # metemos la lista de palabras de la reseña tokenizada \n",
    "                                             # a la lista correspondiente\n",
    "    i=i+1\n",
    "    \n",
    "    \n",
    "    for token in tokens:                     # por cada token de la reseña\n",
    "        if token not in word_index_map:            # verifico si NO esta en el diccionario\n",
    "            word_index_map[token] = current_index       # entonces lo incluyo la palabra en el dicc y su indice\n",
    "            current_index += 1                          # e incremento el contador\n",
    "            \n",
    "print \"numero de reseñas positivas procesadas= \",i\n",
    "print \"Se han cargado hasta ahora \", current_index, \" palabras/tokens en el diccionario.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hago lo mismo pero para las NEGATIVAS, pero en el mismo diccionario\n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuestro diccionario ahora tiene: 11088 palabras/tokens de las reseñas positivas y negativas.\n"
     ]
    }
   ],
   "source": [
    "print \"Nuestro diccionario ahora tiene:\", len(word_index_map), \"palabras/tokens de las reseñas positivas y negativas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11088"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todo_diccionario= word_index_map.items\n",
    "len(todo_diccionario())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asi nos quedaria el word_index_map\n",
    "\n",
    "![word_index_map](word_index_map3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Si el diccionario tiene esta longitud, significa que nuestra representación vectorial al menos tiene este número de dimensiones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de las matrices de Entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cual es el tamaño de nuestros datos de entrada??\n",
    "\n",
    "Una cosa es el tamaño del diccionario que define el numero de dimensiones de la representación vectorial y otra es el tamaño de las muestras, en este caso definido por el numero total de reseñas (positivas + negativas).\n",
    "\n",
    "Como hicimos un balance de reseñas, tenemos 2.000 en total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de reseñas tokenizadas de las listas de reseñas positivas y negativas, entonces:\n",
    "\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos la matriz de los datos de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos ver que es una lista de listas\n",
    "#for tokens in positive_tokenized:\n",
    "#    print tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creamos una función que transforma tokens en vectores\n",
    "\n",
    "# Recibe una lista de palabras o tokens de la reseña y a etiqueta (en este caso positiva o negativa)\n",
    "# Para cada palabra\n",
    "#       busca el indice que le corresponde en el diccionario\n",
    "#       incrementa el elemento del vector que va a representar a la reseña\n",
    "#\n",
    "# Una vez a recorrido todas las palabras de la reseña, normaliza los valores del vector.\n",
    "\n",
    "# El vector que queda es un vector bastante sparse con el peso de las palabras de la reseña \n",
    "# y la etiqueta correspondiente\n",
    "\n",
    "def tokens_to_vector(tokens, label):\n",
    "\n",
    "    x = np.zeros(len(word_index_map) + 1) # creamos un vector de nulos del tamaño del diccionario\n",
    "                                          # y agregamos 1 para la etiqueta\n",
    "    \n",
    "    for t in tokens:                      # recorremos los tokens de la reseña\n",
    "        i = word_index_map[t]                  # guardo el indice de la palabra actual en i\n",
    "        x[i] += 1                              # incremento el contador del vector x[] en 1\n",
    "\n",
    "    # Al final del proceso, me queda un vector con el numero de repeticiones de cada palabra en la posición que \n",
    "    # tiene en el diccionario.\n",
    "\n",
    "    # Ahora normalizamos de tal manera que todo sume 1 \n",
    "    x = x / x.sum()                       # normalizamos antes de asignar la etiqueta\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# La matriz de entrada será entonces de N(numero de reseñas) x D(tamaño del diccionario)+1 (para la etiqueta) \n",
    "\n",
    "# (N x D+1) - las mantenemos juntas para poder hacer shuffle despues\n",
    "\n",
    "# primero creamos la matriz con ceros\n",
    "data = np.zeros((N, len(word_index_map) + 1))  ## N x D+1 (mio)\n",
    "resenia = 0\n",
    "\n",
    "# para cada elemento en la lista de positive_tokenized() la transformo en un vector\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[resenia,:] = xy\n",
    "    resenia += 1\n",
    "# para cada elemento en la lista de negative_tokenized() la transformo en un vector\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[resenia,:] = xy\n",
    "    resenia += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 11089)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordamos: 2000 reseñas x 11.089 palabras en el diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data](matriz_data2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los vectores que representan reseñas positivas deben sumar 2 (los datos normalizados  suman 1 + 1 de la etiqueta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1,:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mientras que los vectores que representan reseñas negativas deben sumar 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1999,:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamos con los datos que hemos preparado\n",
    "\n",
    "Nos ha quedado una matriz de 2000x11086+1 cuyas filas representan a las reseñas y su etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 11088)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hacemos shuffle de los datos, separamos los datos de entrada X() de la etiqueta en Y() \n",
    "# y creamos datos de train/test \n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Por ejemplo, tomamos las ultimas 100 para test\n",
    "muestras_train=-100\n",
    "Xtrain = X[:muestras_train,]\n",
    "Ytrain = Y[:muestras_train,]\n",
    "Xtest = X[muestras_train:,]\n",
    "Ytest = Y[muestras_train:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring de clasificacion: 0.71\n"
     ]
    }
   ],
   "source": [
    "# Vamos a aplicar una Regresion Logistica tradicional\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print \"Scoring de clasificacion:\", model.score(Xtest, Ytest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.score(Xtest, Ytest) - Returns the mean accuracy on the given test data and labels.\n",
    "In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11088)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef. max.: 2.7623077368  ------------      coef. min.: -1.88284237992\n"
     ]
    }
   ],
   "source": [
    "print \"coef. max.:\",model.coef_.max(), \" ------------      coef. min.:\",model.coef_.min(), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora podemos ver el peso de las palabras individualmente\n",
    "\n",
    "La idea es ver qué palabras tienen pesos claramente positivos o negativos.\n",
    "\n",
    "Una vez se ha entrenado el modelo, el vector de coeficientes del modelo (en este caso es de 1x11.088) guarda los pesos de cada una de las palabras. Las que son >0 se consideran positivas.  \n",
    "\n",
    "Para esto ponemos un umbral y vemos cuales quedan por encima del umbral y por debajo del umbral negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 79 ] easy 1.7715509217\n",
      "[ 277 ] love 1.13803429404\n",
      "[ 124 ] wa -1.75606303586\n",
      "[ 139 ] price 2.7623077368\n",
      "[ 96 ] quality 1.47881575566\n",
      "[ 198 ] doe -1.15741747796\n",
      "[ 73 ] sound 1.0422022146\n",
      "[ 78 ] n't -1.88284237992\n",
      "[ 545 ] then -1.10140961413\n",
      "[ 143 ] money -1.12186185926\n",
      "[ 273 ] excellent 1.3441428134\n",
      "[ 2803 ] return -1.21804548018\n"
     ]
    }
   ],
   "source": [
    "#Veamos los pesos de cada palabra\n",
    "threshold = 1\n",
    "for word, index in word_index_map.iteritems():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print \"[\",index,\"]\", word, weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
